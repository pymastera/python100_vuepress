import{_ as i,c as e,a,o as n}from"./app-D-bq_jAa.js";const t={};function k(s,l){return n(),e("div",null,l[0]||(l[0]=[a('<h2 id="k最近邻分类" tabindex="-1"><a class="header-anchor" href="#k最近邻分类"><span>k最近邻分类</span></a></h2><p>$k$最近邻（简称kNN，k-Nearest Neighbor）是Cover和Hart在1968年提出的一种简单的监督学习算法，可用于字符识别、文本分类、图像识别等领域。kNN的工作机制非常简单：给定测试样本，基于某种距离度量（如：欧式距离、曼哈顿距离等）找出训练集中与其最接近的$k$个训练样本，然后基于这$k$个“最近邻居”的信息来进行预测。对于分类任务，可以在$k$个最近邻居中选择出现次数最多的类别标签作为预测的结果；对于回归任务，可以使用$k$个最近邻居实际输出（目标值）的平均值作为预测的结果，当然也可以根据距离的远近进行加权平均，距离越近的样本权重值就越大。</p><h3 id="距离的度量" tabindex="-1"><a class="header-anchor" href="#距离的度量"><span>距离的度量</span></a></h3><ol><li>欧氏距离</li></ol><p>$$ d = \\sqrt{\\sum_{k=1}^n(x_{1k}-x_{2k})^2} $$</p><ol start="2"><li>曼哈顿距离</li></ol><p>$$ d = \\sum_{k=1}^n \\mid {x_{1k}-x_{2k}} \\mid $$</p><ol start="3"><li>切比雪夫距离</li></ol><p>$$ d = max(\\mid x_{1k}-x_{2k} \\mid) $$</p><ol start="4"><li>闵可夫斯基距离 <ul><li>当$p=1$时，就是曼哈顿距离</li><li>当$p=2$时，就是欧式距离</li><li>当$p \\to \\infty$时，就是切比雪夫距离</li></ul></li></ol><p>$$ d = \\sqrt[p]{\\sum_{k=1}^n \\mid x_{1k}-x_{2k} \\mid ^p} $$</p><ol start="5"><li>余弦距离 $$ cos(\\theta) = \\frac{\\sum_{k=1}^n x_{1k}x_{2k}}{\\sqrt{\\sum_{k=1}^n x_{1k}^2} \\sqrt{\\sum_{k=1}^n x_{2k}^2}} $$</li></ol><h3 id="鸢尾花数据集介绍" tabindex="-1"><a class="header-anchor" href="#鸢尾花数据集介绍"><span>鸢尾花数据集介绍</span></a></h3><h3 id="knn算法实现" tabindex="-1"><a class="header-anchor" href="#knn算法实现"><span>kNN算法实现</span></a></h3><p>###使用Scikit-learn实现kNN</p><h3 id="算法优缺点" tabindex="-1"><a class="header-anchor" href="#算法优缺点"><span>算法优缺点</span></a></h3><p>优点：</p><ol><li>简单有效</li><li>重新训练代价低</li><li>适合类域交叉样本</li><li>适合大样本分类</li></ol><p>缺点：</p><ol><li>惰性学习</li><li>输出的可解释性不强</li><li>不擅长处理不均衡样本</li><li>计算量比较大</li></ol><h3 id="k值的选择和交叉检验" tabindex="-1"><a class="header-anchor" href="#k值的选择和交叉检验"><span>k值的选择和交叉检验</span></a></h3><p>k值的选择对于kNN算法的结果有非常显著的影响。下面用李航博士的《统计学习方法》一书中的叙述，来对k值的选择加以说明。</p><p>如果选择较小的$k$值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差会减小，只有与输入实例较近（相似的）训练实例才会对预测结果起作用；但缺点是“学习”的估计误差会增大，预测结果会对近邻的实例点非常敏感，如果近邻的实例点刚好是噪声，预测就会出错。换句话说，$k$值的减小就意味着整体模型变得复杂，容易发生<strong>过拟合</strong>。</p><p>如果选择较大的$k$值，就相当于用较大的邻域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测起作用，使预测发生错误。对于$k=N$的极端情况（其中$N$代表所有的训练实例的数量），那么无论输入实例是什么，都会预测它属于训练实例中最多的类，很显然，这样的模型完全忽略了训练实例中大量的有用信息，是不可取的。</p><p>实际应用中，$k$的取值通常都比较小，可以通过交叉检验的方式来选择较好的$k$值。</p>',25)]))}const $=i(t,[["render",k],["__file","82.k最近邻算法.html.vue"]]),p=JSON.parse('{"path":"/Day81-90/82.k%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95.html","title":"","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"k最近邻分类","slug":"k最近邻分类","link":"#k最近邻分类","children":[{"level":3,"title":"距离的度量","slug":"距离的度量","link":"#距离的度量","children":[]},{"level":3,"title":"鸢尾花数据集介绍","slug":"鸢尾花数据集介绍","link":"#鸢尾花数据集介绍","children":[]},{"level":3,"title":"kNN算法实现","slug":"knn算法实现","link":"#knn算法实现","children":[]},{"level":3,"title":"算法优缺点","slug":"算法优缺点","link":"#算法优缺点","children":[]},{"level":3,"title":"k值的选择和交叉检验","slug":"k值的选择和交叉检验","link":"#k值的选择和交叉检验","children":[]}]}],"git":{"updatedTime":null,"contributors":[]},"filePathRelative":"Day81-90/82.k最近邻算法.md"}');export{$ as comp,p as data};
