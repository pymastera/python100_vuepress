<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.18" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme='dark'] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background-color: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme')
      const systemDarkMode =
        'matchMedia' in window
          ? window.matchMedia('(prefers-color-scheme: dark)').matches
          : false

      if (userMode === 'light') {
        document.documentElement.dataset.theme = 'light'
      } else if (userMode === 'dark' || systemDarkMode) {
        document.documentElement.dataset.theme = 'dark'
      }
    </script>
    <title></title><meta name="description" content="">
    <link rel="preload" href="/assets/style-PoWje89h.css" as="style"><link rel="stylesheet" href="/assets/style-PoWje89h.css">
    <link rel="modulepreload" href="/assets/app-D-bq_jAa.js"><link rel="modulepreload" href="/assets/65.爬虫框架Scrapy简介.html-W8heCdhA.js">
    <link rel="prefetch" href="/assets/index.html-gliNvsHQ.js" as="script"><link rel="prefetch" href="/assets/01.初识Python.html-BwFdoIKy.js" as="script"><link rel="prefetch" href="/assets/02.语言元素.html-CYMl2Yed.js" as="script"><link rel="prefetch" href="/assets/03.分支结构.html-BSYx_tCp.js" as="script"><link rel="prefetch" href="/assets/04.循环结构.html-BJZpY12e.js" as="script"><link rel="prefetch" href="/assets/05.构造程序逻辑.html-BQzZubpL.js" as="script"><link rel="prefetch" href="/assets/06.函数和模块的使用.html-D2HnWKET.js" as="script"><link rel="prefetch" href="/assets/07.字符串和常用数据结构.html-CKl8znKO.js" as="script"><link rel="prefetch" href="/assets/08.面向对象编程基础.html-D_lfcD_q.js" as="script"><link rel="prefetch" href="/assets/09.面向对象进阶.html-2lzuIX0a.js" as="script"><link rel="prefetch" href="/assets/10.图形用户界面和游戏开发.html-By17OOos.js" as="script"><link rel="prefetch" href="/assets/11.文件和异常.html-CgLQgdd8.js" as="script"><link rel="prefetch" href="/assets/12.字符串和正则表达式.html-BPI-RzQV.js" as="script"><link rel="prefetch" href="/assets/13.进程和线程.html-cUulhaPG.js" as="script"><link rel="prefetch" href="/assets/14.网络编程入门和网络应用开发.html-BtJ52ysb.js" as="script"><link rel="prefetch" href="/assets/15.图像和办公文档处理.html-CmKSYg3w.js" as="script"><link rel="prefetch" href="/assets/21-30.Web前端概述.html-BQnr2GML.js" as="script"><link rel="prefetch" href="/assets/16-20.Python语言进阶.html-B6OMzyDF.js" as="script"><link rel="prefetch" href="/assets/31-35.玩转Linux操作系统.html-ZmKmt6al.js" as="script"><link rel="prefetch" href="/assets/46.Django快速上手.html-ClDn9mK8.js" as="script"><link rel="prefetch" href="/assets/47.深入模型.html-BeXTv4Gh.js" as="script"><link rel="prefetch" href="/assets/48.静态资源和Ajax请求.html-CBsfW0m5.js" as="script"><link rel="prefetch" href="/assets/49.Cookie和Session.html-OKxZdF5r.js" as="script"><link rel="prefetch" href="/assets/50.制作报表.html-CHohPMoE.js" as="script"><link rel="prefetch" href="/assets/51.日志和调试工具栏.html-IHfUTr24.js" as="script"><link rel="prefetch" href="/assets/52.中间件的应用.html-EITG4bC6.js" as="script"><link rel="prefetch" href="/assets/53.前后端分离开发入门.html-CSfpFnpc.js" as="script"><link rel="prefetch" href="/assets/54.RESTful架构和DRF入门.html-unYp2iVf.js" as="script"><link rel="prefetch" href="/assets/55.RESTful架构和DRF进阶.html-Bimn41qo.js" as="script"><link rel="prefetch" href="/assets/56.使用缓存.html-DygStMaf.js" as="script"><link rel="prefetch" href="/assets/57.接入三方平台.html-BOYDJBOY.js" as="script"><link rel="prefetch" href="/assets/58.异步任务和定时任务.html-Df3PFO5Q.js" as="script"><link rel="prefetch" href="/assets/59.单元测试.html-DsPevIDF.js" as="script"><link rel="prefetch" href="/assets/60.项目上线.html-DMRv6p1t.js" as="script"><link rel="prefetch" href="/assets/36.关系型数据库和MySQL概述.html-BxE9DXEh.js" as="script"><link rel="prefetch" href="/assets/37.SQL详解之DDL.html-Bgp1k0eR.js" as="script"><link rel="prefetch" href="/assets/38.SQL详解之DML.html-uimrZls-.js" as="script"><link rel="prefetch" href="/assets/39.SQL详解之DQL.html-Ddo7fJHv.js" as="script"><link rel="prefetch" href="/assets/40.SQL详解之DCL.html-CE0er4AB.js" as="script"><link rel="prefetch" href="/assets/41.MySQL新特性.html-Be23vhPq.js" as="script"><link rel="prefetch" href="/assets/42.视图、函数和过程.html-JJRxgqze.js" as="script"><link rel="prefetch" href="/assets/43.索引.html-DCuE2oxz.js" as="script"><link rel="prefetch" href="/assets/44.Python接入MySQL数据库.html-CDHCppw7.js" as="script"><link rel="prefetch" href="/assets/45.大数据平台和HiveSQL.html-BoScJKu1.js" as="script"><link rel="prefetch" href="/assets/66.数据分析概述.html-BnwcA8AB.js" as="script"><link rel="prefetch" href="/assets/67.环境准备.html-DHxqV8P9.js" as="script"><link rel="prefetch" href="/assets/68.NumPy的应用-1.html-B3M5Stew.js" as="script"><link rel="prefetch" href="/assets/69.NumPy的应用-2.html-DINfPSOD.js" as="script"><link rel="prefetch" href="/assets/70.NumPy的应用-3.html-Cq0BS2Tg.js" as="script"><link rel="prefetch" href="/assets/71.NumPy的应用-4.html-BONDNBne.js" as="script"><link rel="prefetch" href="/assets/72.深入浅出pandas-1.html-mfHBd5ix.js" as="script"><link rel="prefetch" href="/assets/73.深入浅出pandas-2.html-BzB_BqJB.js" as="script"><link rel="prefetch" href="/assets/74.深入浅出pandas-3.html-CsRWNuji.js" as="script"><link rel="prefetch" href="/assets/75.深入浅出pandas-4.html-BvqbzoEe.js" as="script"><link rel="prefetch" href="/assets/76.深入浅出pandas-5.html-ukdSbkgj.js" as="script"><link rel="prefetch" href="/assets/77.深入浅出pandas-6.html-Cb21IcVJ.js" as="script"><link rel="prefetch" href="/assets/78.数据可视化-1.html-B7sXt-G5.js" as="script"><link rel="prefetch" href="/assets/79.数据可视化-2.html-Bv_oTn1i.js" as="script"><link rel="prefetch" href="/assets/80.数据可视化-3.html-BLZsn4HV.js" as="script"><link rel="prefetch" href="/assets/61.网络数据采集概述.html-CjR5vzG3.js" as="script"><link rel="prefetch" href="/assets/62.用Python获取网络资源-1.html-BKM7qrLE.js" as="script"><link rel="prefetch" href="/assets/62.用Python解析HTML页面-2.html-Dtcd-kct.js" as="script"><link rel="prefetch" href="/assets/63.Python中的并发编程-1.html-BekWYpsa.js" as="script"><link rel="prefetch" href="/assets/63.Python中的并发编程-2.html-CVXcHBzj.js" as="script"><link rel="prefetch" href="/assets/63.Python中的并发编程-3.html-BlfLRjkc.js" as="script"><link rel="prefetch" href="/assets/63.并发编程在爬虫中的应用.html-ti0e-1h5.js" as="script"><link rel="prefetch" href="/assets/64.使用Selenium抓取网页动态内容.html-BLFVKpq_.js" as="script"><link rel="prefetch" href="/assets/81.人工智能和机器学习概述.html-Ca7AYT__.js" as="script"><link rel="prefetch" href="/assets/82.k最近邻算法.html-CGQnIPO4.js" as="script"><link rel="prefetch" href="/assets/83.决策树.html-BwWQjZ__.js" as="script"><link rel="prefetch" href="/assets/84.聚类算法.html-BnETr6ij.js" as="script"><link rel="prefetch" href="/assets/85.朴素贝叶斯算法.html-Rct6EKZw.js" as="script"><link rel="prefetch" href="/assets/86.支持向量机.html-yAGpiCbO.js" as="script"><link rel="prefetch" href="/assets/87.回归分析.html-BO-2cKfh.js" as="script"><link rel="prefetch" href="/assets/88.深度学习入门.html-BOSrbht3.js" as="script"><link rel="prefetch" href="/assets/89.PyTorch概述.html-BXDTY1W0.js" as="script"><link rel="prefetch" href="/assets/90.PyTorch实战.html-DWYNWcjH.js" as="script"><link rel="prefetch" href="/assets/100.Python面试题实录.html-C5EnR3Ed.js" as="script"><link rel="prefetch" href="/assets/91.团队项目开发的问题和解决方案.html-CECcJy8n.js" as="script"><link rel="prefetch" href="/assets/92.Docker容器技术详解.html-DGacXk8p.js" as="script"><link rel="prefetch" href="/assets/93.MySQL性能优化.html-BChvKIt4.js" as="script"><link rel="prefetch" href="/assets/94.网络API接口设计.html-ChQlRSGo.js" as="script"><link rel="prefetch" href="/assets/95.使用Django开发商业项目.html-DECQMK9m.js" as="script"><link rel="prefetch" href="/assets/96.软件测试和自动化测试.html-D3r3us4u.js" as="script"><link rel="prefetch" href="/assets/97.电商网站技术要点剖析.html-hB1D5G1G.js" as="script"><link rel="prefetch" href="/assets/98.项目部署上线和性能调优.html-CT3LhWhJ.js" as="script"><link rel="prefetch" href="/assets/99.面试中的公共问题.html-DMTzlILS.js" as="script"><link rel="prefetch" href="/assets/PEP8风格指南.html-DN3Ws3cc.js" as="script"><link rel="prefetch" href="/assets/Python之禅的最佳翻译.html-TKNJu8cq.js" as="script"><link rel="prefetch" href="/assets/Python参考书籍.html-CO-ccpF4.js" as="script"><link rel="prefetch" href="/assets/Python容器使用小技巧.html-DZR49nel.js" as="script"><link rel="prefetch" href="/assets/Python数据分析师面试题.html-YtNemgMP.js" as="script"><link rel="prefetch" href="/assets/Python编程惯例.html-CvmEKn9m.js" as="script"><link rel="prefetch" href="/assets/一个小例子助你彻底理解协程.html-Cq1CFFHT.js" as="script"><link rel="prefetch" href="/assets/使用Hexo搭建自己的博客.html-CXPev5HK.js" as="script"><link rel="prefetch" href="/assets/常见反爬策略及应对方案.html-DCecg34L.js" as="script"><link rel="prefetch" href="/assets/我为什么选择了Python.html-BuUPym1o.js" as="script"><link rel="prefetch" href="/assets/接口文档参考示例.html-GRoBovI1.js" as="script"><link rel="prefetch" href="/assets/玩转PyCharm.html--XkJxH2_.js" as="script"><link rel="prefetch" href="/assets/用函数还是用复杂的表达式.html-2n5kS7oc.js" as="script"><link rel="prefetch" href="/assets/知乎问题回答.html-CkWOFmLg.js" as="script"><link rel="prefetch" href="/assets/英语面试.html-CZGJtC3Y.js" as="script"><link rel="prefetch" href="/assets/那些年我们踩过的那些坑.html-C1BI6oD2.js" as="script"><link rel="prefetch" href="/assets/年薪50W_的Python程序员如何写代码.html-C49SuvhI.js" as="script"><link rel="prefetch" href="/assets/好玩的Python.html-CpBhFJ_2.js" as="script"><link rel="prefetch" href="/assets/算法入门系列1-周而复始.html-32WRx0Tf.js" as="script"><link rel="prefetch" href="/assets/算法入门系列2 - 在水一方.html-ChrbgX52.js" as="script"><link rel="prefetch" href="/assets/404.html-8giqv0M_.js" as="script"><link rel="prefetch" href="/assets/setupDevtools-7MC2TMWH-CexIHObO.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="vp-theme-container external-link-icon" vp-container><!--[--><header class="vp-navbar" vp-navbar><div class="vp-toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/"><!----><!----></a></span><div class="vp-navbar-items-wrapper" style=""><!--[--><!--]--><!----><!--[--><!--]--><button type="button" class="vp-toggle-color-mode-button" title="toggle color mode"><svg class="light-icon" viewbox="0 0 32 32" style=""><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg class="dark-icon" viewbox="0 0 32 32" style="display:none;"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="vp-sidebar-mask"></div><!--[--><aside class="vp-sidebar" vp-sidebar><!----><!--[--><!--]--><ul class="vp-sidebar-items"><!--[--><li><p tabindex="0" class="vp-sidebar-item vp-sidebar-heading"> <!----></p><!----></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="vp-page"><!--[--><!--]--><div class="theme-default-content" vp-content><!--[--><!--]--><div><h2 id="爬虫框架scrapy简介" tabindex="-1"><a class="header-anchor" href="#爬虫框架scrapy简介"><span>爬虫框架Scrapy简介</span></a></h2><p>当你写了很多个爬虫程序之后，你会发现每次写爬虫程序时，都需要将页面获取、页面解析、爬虫调度、异常处理、反爬应对这些代码从头至尾实现一遍，这里面有很多工作其实都是简单乏味的重复劳动。那么，有没有什么办法可以提升我们编写爬虫代码的效率呢？答案是肯定的，那就是利用爬虫框架，而在所有的爬虫框架中，Scrapy 应该是最流行、最强大的框架。</p><h3 id="scrapy-概述" tabindex="-1"><a class="header-anchor" href="#scrapy-概述"><span>Scrapy 概述</span></a></h3><p>Scrapy 是基于 Python 的一个非常流行的网络爬虫框架，可以用来抓取 Web 站点并从页面中提取结构化的数据。下图展示了 Scrapy 的基本架构，其中包含了主要组件和系统的数据处理流程（图中带数字的红色箭头）。</p><p><img src="https://gitee.com/jackfrued/mypic/raw/master/20210824003638.png" alt=""></p><h4 id="scrapy的组件" tabindex="-1"><a class="header-anchor" href="#scrapy的组件"><span>Scrapy的组件</span></a></h4><p>我们先来说说 Scrapy 中的组件。</p><ol><li>Scrapy 引擎（Engine）：用来控制整个系统的数据处理流程。</li><li>调度器（Scheduler）：调度器从引擎接受请求并排序列入队列，并在引擎发出请求后返还给它们。</li><li>下载器（Downloader）：下载器的主要职责是抓取网页并将网页内容返还给蜘蛛（Spiders）。</li><li>蜘蛛程序（Spiders）：蜘蛛是用户自定义的用来解析网页并抓取特定URL的类，每个蜘蛛都能处理一个域名或一组域名，简单的说就是用来定义特定网站的抓取和解析规则的模块。</li><li>数据管道（Item Pipeline）：管道的主要责任是负责处理有蜘蛛从网页中抽取的数据条目，它的主要任务是清理、验证和存储数据。当页面被蜘蛛解析后，将被发送到数据管道，并经过几个特定的次序处理数据。每个数据管道组件都是一个 Python 类，它们获取了数据条目并执行对数据条目进行处理的方法，同时还需要确定是否需要在数据管道中继续执行下一步或是直接丢弃掉不处理。数据管道通常执行的任务有：清理 HTML 数据、验证解析到的数据（检查条目是否包含必要的字段）、检查是不是重复数据（如果重复就丢弃）、将解析到的数据存储到数据库（关系型数据库或 NoSQL 数据库）中。</li><li>中间件（Middlewares）：中间件是介于引擎和其他组件之间的一个钩子框架，主要是为了提供自定义的代码来拓展 Scrapy 的功能，包括下载器中间件和蜘蛛中间件。</li></ol><h4 id="数据处理流程" tabindex="-1"><a class="header-anchor" href="#数据处理流程"><span>数据处理流程</span></a></h4><p>Scrapy 的整个数据处理流程由引擎进行控制，通常的运转流程包括以下的步骤：</p><ol><li><p>引擎询问蜘蛛需要处理哪个网站，并让蜘蛛将第一个需要处理的 URL 交给它。</p></li><li><p>引擎让调度器将需要处理的 URL 放在队列中。</p></li><li><p>引擎从调度那获取接下来进行爬取的页面。</p></li><li><p>调度将下一个爬取的 URL 返回给引擎，引擎将它通过下载中间件发送到下载器。</p></li><li><p>当网页被下载器下载完成以后，响应内容通过下载中间件被发送到引擎；如果下载失败了，引擎会通知调度器记录这个 URL，待会再重新下载。</p></li><li><p>引擎收到下载器的响应并将它通过蜘蛛中间件发送到蜘蛛进行处理。</p></li><li><p>蜘蛛处理响应并返回爬取到的数据条目，此外还要将需要跟进的新的 URL 发送给引擎。</p></li><li><p>引擎将抓取到的数据条目送入数据管道，把新的 URL 发送给调度器放入队列中。</p></li></ol><p>上述操作中的第2步到第8步会一直重复直到调度器中没有需要请求的 URL，爬虫就停止工作。</p><h3 id="安装和使用scrapy" tabindex="-1"><a class="header-anchor" href="#安装和使用scrapy"><span>安装和使用Scrapy</span></a></h3><p>可以使用 Python 的包管理工具<code>pip</code>来安装 Scrapy。</p><div class="language-Shell line-numbers-mode" data-highlighter="prismjs" data-ext="Shell" data-title="Shell"><pre><code><span class="line">pip install scrapy</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>在命令行中使用<code>scrapy</code>命令创建名为<code>demo</code>的项目。</p><div class="language-Bash line-numbers-mode" data-highlighter="prismjs" data-ext="Bash" data-title="Bash"><pre><code><span class="line">scrapy startproject demo</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>项目的目录结构如下图所示。</p><div class="language-Shell line-numbers-mode" data-highlighter="prismjs" data-ext="Shell" data-title="Shell"><pre><code><span class="line">demo</span>
<span class="line">|____ demo</span>
<span class="line">|________ spiders</span>
<span class="line">|____________ __init__.py</span>
<span class="line">|________ __init__.py</span>
<span class="line">|________ items.py</span>
<span class="line">|________ middlewares.py</span>
<span class="line">|________ pipelines.py</span>
<span class="line">|________ settings.py</span>
<span class="line">|____ scrapy.cfg</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>切换到<code>demo</code> 目录，用下面的命令创建名为<code>douban</code>的蜘蛛程序。</p><div class="language-Bash line-numbers-mode" data-highlighter="prismjs" data-ext="Bash" data-title="Bash"><pre><code><span class="line">scrapy genspider douban movie.douban.com</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h4 id="一个简单的例子" tabindex="-1"><a class="header-anchor" href="#一个简单的例子"><span>一个简单的例子</span></a></h4><p>接下来，我们实现一个爬取豆瓣电影 Top250 电影标题、评分和金句的爬虫。</p><ol><li><p>在<code>items.py</code>的<code>Item</code>类中定义字段，这些字段用来保存数据，方便后续的操作。</p><div class="language-Python line-numbers-mode" data-highlighter="prismjs" data-ext="Python" data-title="Python"><pre><code><span class="line">import scrapy</span>
<span class="line"></span>
<span class="line"></span>
<span class="line">class DoubanItem(scrapy.Item):</span>
<span class="line">    title = scrapy.Field()</span>
<span class="line">    score = scrapy.Field()</span>
<span class="line">    motto = scrapy.Field()</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>修改<code>spiders</code>文件夹中名为<code>douban.py</code> 的文件，它是蜘蛛程序的核心，需要我们添加解析页面的代码。在这里，我们可以通过对<code>Response</code>对象的解析，获取电影的信息，代码如下所示。</p><div class="language-Python line-numbers-mode" data-highlighter="prismjs" data-ext="Python" data-title="Python"><pre><code><span class="line">import scrapy</span>
<span class="line">from scrapy import Selector, Request</span>
<span class="line">from scrapy.http import HtmlResponse</span>
<span class="line"></span>
<span class="line">from demo.items import MovieItem</span>
<span class="line"></span>
<span class="line"></span>
<span class="line">class DoubanSpider(scrapy.Spider):</span>
<span class="line">    name = &#39;douban&#39;</span>
<span class="line">    allowed_domains = [&#39;movie.douban.com&#39;]</span>
<span class="line">    start_urls = [&#39;https://movie.douban.com/top250?start=0&amp;filter=&#39;]</span>
<span class="line"></span>
<span class="line">    def parse(self, response: HtmlResponse):</span>
<span class="line">        sel = Selector(response)</span>
<span class="line">        movie_items = sel.css(&#39;#content &gt; div &gt; div.article &gt; ol &gt; li&#39;)</span>
<span class="line">        for movie_sel in movie_items:</span>
<span class="line">            item = MovieItem()</span>
<span class="line">            item[&#39;title&#39;] = movie_sel.css(&#39;.title::text&#39;).extract_first()</span>
<span class="line">            item[&#39;score&#39;] = movie_sel.css(&#39;.rating_num::text&#39;).extract_first()</span>
<span class="line">            item[&#39;motto&#39;] = movie_sel.css(&#39;.inq::text&#39;).extract_first()</span>
<span class="line">            yield item</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>通过上面的代码不难看出，我们可以使用 CSS 选择器进行页面解析。当然，如果你愿意也可以使用 XPath 或正则表达式进行页面解析，对应的方法分别是<code>xpath</code>和<code>re</code>。</p><p>如果还要生成后续爬取的请求，我们可以用<code>yield</code>产出<code>Request</code>对象。<code>Request</code>对象有两个非常重要的属性，一个是<code>url</code>，它代表了要请求的地址；一个是<code>callback</code>，它代表了获得响应之后要执行的回调函数。我们可以将上面的代码稍作修改。</p><div class="language-Python line-numbers-mode" data-highlighter="prismjs" data-ext="Python" data-title="Python"><pre><code><span class="line">import scrapy</span>
<span class="line">from scrapy import Selector, Request</span>
<span class="line">from scrapy.http import HtmlResponse</span>
<span class="line"></span>
<span class="line">from demo.items import MovieItem</span>
<span class="line"></span>
<span class="line"></span>
<span class="line">class DoubanSpider(scrapy.Spider):</span>
<span class="line">    name = &#39;douban&#39;</span>
<span class="line">    allowed_domains = [&#39;movie.douban.com&#39;]</span>
<span class="line">    start_urls = [&#39;https://movie.douban.com/top250?start=0&amp;filter=&#39;]</span>
<span class="line"></span>
<span class="line">    def parse(self, response: HtmlResponse):</span>
<span class="line">        sel = Selector(response)</span>
<span class="line">        movie_items = sel.css(&#39;#content &gt; div &gt; div.article &gt; ol &gt; li&#39;)</span>
<span class="line">        for movie_sel in movie_items:</span>
<span class="line">            item = MovieItem()</span>
<span class="line">            item[&#39;title&#39;] = movie_sel.css(&#39;.title::text&#39;).extract_first()</span>
<span class="line">            item[&#39;score&#39;] = movie_sel.css(&#39;.rating_num::text&#39;).extract_first()</span>
<span class="line">            item[&#39;motto&#39;] = movie_sel.css(&#39;.inq::text&#39;).extract_first()</span>
<span class="line">            yield item</span>
<span class="line"></span>
<span class="line">        hrefs = sel.css(&#39;#content &gt; div &gt; div.article &gt; div.paginator &gt; a::attr(&quot;href&quot;)&#39;)</span>
<span class="line">        for href in hrefs:</span>
<span class="line">            full_url = response.urljoin(href.extract())</span>
<span class="line">            yield Request(url=full_url)</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>到这里，我们已经可以通过下面的命令让爬虫运转起来。</p><div class="language-Shell line-numbers-mode" data-highlighter="prismjs" data-ext="Shell" data-title="Shell"><pre><code><span class="line">scrapy crawl movie</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>可以在控制台看到爬取到的数据，如果想将这些数据保存到文件中，可以通过<code>-o</code>参数来指定文件名，Scrapy 支持我们将爬取到的数据导出成 JSON、CSV、XML 等格式。</p><div class="language-Shell line-numbers-mode" data-highlighter="prismjs" data-ext="Shell" data-title="Shell"><pre><code><span class="line">scrapy crawl moive -o result.json</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>不知大家是否注意到，通过运行爬虫获得的 JSON 文件中有<code>275</code>条数据，那是因为首页被重复爬取了。要解决这个问题，可以对上面的代码稍作调整，不在<code>parse</code>方法中解析获取新页面的 URL，而是通过<code>start_requests</code>方法提前准备好待爬取页面的 URL，调整后的代码如下所示。</p><div class="language-Python line-numbers-mode" data-highlighter="prismjs" data-ext="Python" data-title="Python"><pre><code><span class="line">import scrapy</span>
<span class="line">from scrapy import Selector, Request</span>
<span class="line">from scrapy.http import HtmlResponse</span>
<span class="line"></span>
<span class="line">from demo.items import MovieItem</span>
<span class="line"></span>
<span class="line"></span>
<span class="line">class DoubanSpider(scrapy.Spider):</span>
<span class="line">    name = &#39;douban&#39;</span>
<span class="line">    allowed_domains = [&#39;movie.douban.com&#39;]</span>
<span class="line"></span>
<span class="line">    def start_requests(self):</span>
<span class="line">        for page in range(10):</span>
<span class="line">            yield Request(url=f&#39;https://movie.douban.com/top250?start={page * 25}&#39;)</span>
<span class="line"></span>
<span class="line">    def parse(self, response: HtmlResponse):</span>
<span class="line">        sel = Selector(response)</span>
<span class="line">        movie_items = sel.css(&#39;#content &gt; div &gt; div.article &gt; ol &gt; li&#39;)</span>
<span class="line">        for movie_sel in movie_items:</span>
<span class="line">            item = MovieItem()</span>
<span class="line">            item[&#39;title&#39;] = movie_sel.css(&#39;.title::text&#39;).extract_first()</span>
<span class="line">            item[&#39;score&#39;] = movie_sel.css(&#39;.rating_num::text&#39;).extract_first()</span>
<span class="line">            item[&#39;motto&#39;] = movie_sel.css(&#39;.inq::text&#39;).extract_first()</span>
<span class="line">            yield item</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>如果希望完成爬虫数据的持久化，可以在数据管道中处理蜘蛛程序产生的<code>Item</code>对象。例如，我们可以通过前面讲到的<code>openpyxl</code>操作 Excel 文件，将数据写入 Excel 文件中，代码如下所示。</p><div class="language-Python line-numbers-mode" data-highlighter="prismjs" data-ext="Python" data-title="Python"><pre><code><span class="line">import openpyxl</span>
<span class="line"></span>
<span class="line">from demo.items import MovieItem</span>
<span class="line"></span>
<span class="line"></span>
<span class="line">class MovieItemPipeline:</span>
<span class="line"></span>
<span class="line">    def __init__(self):</span>
<span class="line">        self.wb = openpyxl.Workbook()</span>
<span class="line">        self.sheet = self.wb.active</span>
<span class="line">        self.sheet.title = &#39;Top250&#39;</span>
<span class="line">        self.sheet.append((&#39;名称&#39;, &#39;评分&#39;, &#39;名言&#39;))</span>
<span class="line"></span>
<span class="line">    def process_item(self, item: MovieItem, spider):</span>
<span class="line">        self.sheet.append((item[&#39;title&#39;], item[&#39;score&#39;], item[&#39;motto&#39;]))</span>
<span class="line">        return item</span>
<span class="line"></span>
<span class="line">    def close_spider(self, spider):</span>
<span class="line">        self.wb.save(&#39;豆瓣电影数据.xlsx&#39;)</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>上面的<code>process_item</code>和<code>close_spider</code>都是回调方法（钩子函数）， 简单的说就是 Scrapy 框架会自动去调用的方法。当蜘蛛程序产生一个<code>Item</code>对象交给引擎时，引擎会将该<code>Item</code>对象交给数据管道，这时我们配置好的数据管道的<code>parse_item</code>方法就会被执行，所以我们可以在该方法中获取数据并完成数据的持久化操作。另一个方法<code>close_spider</code>是在爬虫结束运行前会自动执行的方法，在上面的代码中，我们在这个地方进行了保存 Excel 文件的操作，相信这段代码大家是很容易读懂的。</p><p>总而言之，数据管道可以帮助我们完成以下操作：</p><ul><li>清理 HTML 数据，验证爬取的数据。</li><li>丢弃重复的不必要的内容。</li><li>将爬取的结果进行持久化操作。</li></ul></li><li><p>修改<code>settings.py</code>文件对项目进行配置，主要需要修改以下几个配置。</p><div class="language-Python line-numbers-mode" data-highlighter="prismjs" data-ext="Python" data-title="Python"><pre><code><span class="line"># 用户浏览器</span>
<span class="line">USER_AGENT = &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36&#39;</span>
<span class="line"></span>
<span class="line"># 并发请求数量 </span>
<span class="line">CONCURRENT_REQUESTS = 4</span>
<span class="line"></span>
<span class="line"># 下载延迟</span>
<span class="line">DOWNLOAD_DELAY = 3</span>
<span class="line"># 随机化下载延迟</span>
<span class="line">RANDOMIZE_DOWNLOAD_DELAY = True</span>
<span class="line"></span>
<span class="line"># 是否遵守爬虫协议</span>
<span class="line">ROBOTSTXT_OBEY = True</span>
<span class="line"></span>
<span class="line"># 配置数据管道</span>
<span class="line">ITEM_PIPELINES = {</span>
<span class="line">   &#39;demo.pipelines.MovieItemPipeline&#39;: 300,</span>
<span class="line">}</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p><strong>说明</strong>：上面配置文件中的<code>ITEM_PIPELINES</code>选项是一个字典，可以配置多个处理数据的管道，后面的数字代表了执行的优先级，数字小的先执行。</p></blockquote></li></ol></div><!--[--><!--]--></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><!----><!----></div></footer><!----><!--[--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-D-bq_jAa.js" defer></script>
  </body>
</html>
